{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training data clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import us as usStates\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import savetxt\n",
    "from datetime import datetime, timedelta\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "import json\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "\n",
    "# def days(d1, d2):\n",
    "#     d1 = datetime.strptime(d1, \"%m/%d/%Y\")\n",
    "#     d2 = datetime.strptime(d2, \"%m/%d/%Y\")\n",
    "#     return abs((d2 - d1).days)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the Gleam predictions (point esitimate) and calculate the ML Gleam predictions (23 quantiles) \n",
    "for the next four week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('../configs/data_cfg/data_cfg.json')\n",
    "file = json.load(f)\n",
    "gleam_path = file['gleam_path']\n",
    "result_path = file['result_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9792, 6)\n",
      "(4896, 6)\n",
      "(4800, 6)\n",
      "(200, 6)\n"
     ]
    }
   ],
   "source": [
    "df_origin = pd.read_csv(gleam_path+'2021-01-18-MOBS-GLEAM_COVID.csv')\n",
    "df_origin = df_origin.fillna('NA')\n",
    "df = df_origin.sort_values(by=['location','target','quantile'])\n",
    "df1 = df.drop('forecast_date', axis=1)\n",
    "print(df1.shape)\n",
    "df2 = df1[(df1.target != '1 wk ahead cum death') & (df1.target != '2 wk ahead cum death')\n",
    "   & (df1.target != '3 wk ahead cum death')& (df1.target != '4 wk ahead cum death')]\n",
    "df2 = df1.loc[df1['target'].isin(['1 wk ahead inc death','2 wk ahead inc death',\n",
    "                                 '3 wk ahead inc death','4 wk ahead inc death'])]\n",
    "df3 = df2.loc[df2['location'] != 'US']\n",
    "df4 = df3.loc[df3['type'] == 'point']\n",
    "print(df2.shape)\n",
    "print(df3.shape)\n",
    "print(df4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 1)\n",
      "(50, 1)\n",
      "(50, 1)\n",
      "(50, 1)\n"
     ]
    }
   ],
   "source": [
    "df_1week = df4.loc[df4['target_end_date'] == '2021-01-23']\n",
    "df_2week = df4.loc[df4['target_end_date'] == '2021-01-30']\n",
    "df_3week = df4.loc[df4['target_end_date'] == '2021-02-06']\n",
    "df_4week = df4.loc[df4['target_end_date'] == '2021-02-13']\n",
    "\n",
    "df_1week = df_1week.drop(['target','target_end_date','type','quantile'], axis=1)\n",
    "df_2week = df_2week.drop(['target','target_end_date','type','quantile'], axis=1)\n",
    "df_3week = df_3week.drop(['target','target_end_date','type','quantile'], axis=1)\n",
    "df_4week = df_4week.drop(['target','target_end_date','type','quantile'], axis=1)\n",
    "\n",
    "\n",
    "Gleam_week1 = np.expand_dims(df_1week['value'].to_numpy(),axis=1)\n",
    "Gleam_week2 = np.expand_dims(df_2week['value'].to_numpy(),axis=1)\n",
    "Gleam_week3 = np.expand_dims(df_3week['value'].to_numpy(),axis=1)\n",
    "Gleam_week4 = np.expand_dims(df_4week['value'].to_numpy(),axis=1)\n",
    "\n",
    "print(Gleam_week1.shape)\n",
    "print(Gleam_week2.shape)\n",
    "print(Gleam_week3.shape)\n",
    "print(Gleam_week4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 23)\n",
      "(50, 23)\n",
      "(50, 23)\n",
      "(50, 23)\n"
     ]
    }
   ],
   "source": [
    "ML_residual = np.load(result_path+'ensembledata.npy')\n",
    "# ML_residual = np.load('ensembledata_torchts.npy')\n",
    "    \n",
    "ML_residual1 = ML_residual[0]\n",
    "ML_residual2 = ML_residual[1]\n",
    "ML_residual3 = ML_residual[2]\n",
    "ML_residual4 = ML_residual[3]\n",
    "\n",
    "print(ML_residual1.shape)\n",
    "print(ML_residual2.shape)\n",
    "print(ML_residual3.shape)\n",
    "print(ML_residual4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[15 15 13 11  9  8  6  4  3  3  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "[19 15 15 12 10  9  7  6  4  3  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "[21 18 15 13 11  9  8  6  4  4  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "[22 19 18 13 12 11  9  6  5  4  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "ML_Gleam_death1 = np.flip(Gleam_week1 - ML_residual1,axis =1)\n",
    "ML_Gleam_death2 = np.flip(Gleam_week2 - ML_residual2,axis =1)\n",
    "ML_Gleam_death3 = np.flip(Gleam_week3 - ML_residual3,axis =1)\n",
    "ML_Gleam_death4 = np.flip(Gleam_week4 - ML_residual4,axis =1)\n",
    "\n",
    "print(22*50-sum(sum(np.diff(ML_Gleam_death1)>=0)))\n",
    "print(22*50-sum(sum(np.diff(ML_Gleam_death2)>=0)))\n",
    "print(22*50-sum(sum(np.diff(ML_Gleam_death3)>=0)))\n",
    "print(22*50-sum(sum(np.diff(ML_Gleam_death4)>=0)))\n",
    "\n",
    "print(np.sum(np.array(ML_Gleam_death1) < 0, axis=0))\n",
    "print(np.sum(np.array(ML_Gleam_death2) < 0, axis=0))\n",
    "print(np.sum(np.array(ML_Gleam_death3) < 0, axis=0))\n",
    "print(np.sum(np.array(ML_Gleam_death4) < 0, axis=0))\n",
    "\n",
    "#set to positive\n",
    "ML_Gleam_death1 = np.maximum(ML_Gleam_death1,0)\n",
    "ML_Gleam_death2 = np.maximum(ML_Gleam_death2,0)\n",
    "ML_Gleam_death3 = np.maximum(ML_Gleam_death3,0)\n",
    "ML_Gleam_death4 = np.maximum(ML_Gleam_death4,0)\n",
    "\n",
    "print(np.sum(np.array(ML_Gleam_death1) < 0, axis=0))\n",
    "print(np.sum(np.array(ML_Gleam_death2) < 0, axis=0))\n",
    "print(np.sum(np.array(ML_Gleam_death3) < 0, axis=0))\n",
    "print(np.sum(np.array(ML_Gleam_death4) < 0, axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inc data use sorted ML Leam death\n",
    "# add US. data\n",
    "US_death1 = np.append(ML_Gleam_death1,np.expand_dims(np.sum(ML_Gleam_death1,axis = 0),axis=0),axis = 0)\n",
    "US_death2 = np.append(ML_Gleam_death2,np.expand_dims(np.sum(ML_Gleam_death2,axis = 0),axis=0),axis = 0)\n",
    "US_death3 = np.append(ML_Gleam_death3,np.expand_dims(np.sum(ML_Gleam_death3,axis = 0),axis=0),axis = 0)\n",
    "US_death4 = np.append(ML_Gleam_death4,np.expand_dims(np.sum(ML_Gleam_death4,axis = 0),axis=0),axis = 0)\n",
    "\n",
    "# copy 50% quantile for point\n",
    "inc_death1 = np.insert(US_death1,23,US_death1[:,11],axis = 1)\n",
    "inc_death2 = np.insert(US_death2,23,US_death2[:,11],axis = 1)\n",
    "inc_death3 = np.insert(US_death3,23,US_death3[:,11],axis = 1)\n",
    "inc_death4 = np.insert(US_death4,23,US_death4[:,11],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate the submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  6119    229  11248   4293  33342   5363   6594   1016  24004  12291\n",
      "    319   1605  20020   9287   4321   3489   3093   8080    507   6541\n",
      "  13583  14669   5955   5481   6467   1088   1837   3761    927  20414\n",
      "   2910  40806   8016   1403  10135   2952   1799  19143   2005   6108\n",
      "   1633   8355  32428   1485    163   5706   3903   1761   5905    522\n",
      " 395785]\n"
     ]
    }
   ],
   "source": [
    "# previous death\n",
    "cumweek1 = df1.loc[(df1['target'] =='1 wk ahead cum death') & (df1['type'] == 'point')]\n",
    "incweek1 = df1.loc[(df1['target'] =='1 wk ahead inc death')& (df1['type'] == 'point')]\n",
    "cumweek_1 = cumweek1['value'].to_numpy()\n",
    "incweek_1 = incweek1['value'].to_numpy()\n",
    "previous_death = np.around(cumweek_1 - incweek_1).astype(int)\n",
    "print(previous_death)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cum death\n",
    "cum_death1 = np.add(inc_death1,np.expand_dims(previous_death,axis = 1))\n",
    "cum_death2 = np.add(inc_death2,cum_death1)\n",
    "cum_death3 = np.add(inc_death3,cum_death2)\n",
    "cum_death4 = np.add(inc_death4,cum_death3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9792,)\n"
     ]
    }
   ],
   "source": [
    "# all the data\n",
    "value_data = np.stack([cum_death1,inc_death1,cum_death2,inc_death2,\n",
    "                             cum_death3,inc_death3,cum_death4,inc_death4],axis=1).flatten()\n",
    "# add to the value column\n",
    "print(value_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['value'])\n",
    "df['value'] = value_data\n",
    "\n",
    "df = df.astype({'forecast_date': 'string','target': 'string', 'target_end_date': 'string', 'location': 'string',\n",
    "          'type': 'string', 'location': 'string', 'value': 'float64'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "forecast_date       string\n",
       "target              string\n",
       "target_end_date     string\n",
       "location            string\n",
       "type                string\n",
       "quantile            object\n",
       "value              float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(gleam_path+'2021-01-18-UCSD_NEU-DeepGLEAM.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
